# **4.1.3. Review Webserver Metafiles for Information Leakage** 
---
## *Kiểm tra Metafiles của Web Server, rò rỉ những thông tin nào ?*

**1.Khái quát**
- Cách kiểm tra tệp robots.txt
- Danh sách các thư mục có thể tránh Spiders, Robots, Crawlers, cũng có thể được tạo ra, liên quan tới Map execution paths through application.(Bản đổ thực thi các đường dẫn (path) thông qua Application). 
- Spiders, Robots, Crawlers. 
    - Spiders.txt : Ngăn chặn Search Engine tìm kiếm được ID, nhưng không hạn chế quyền truy cập vào store.
    Một số nguồn nói rằng : tệp này gồm danh sách các tác nhân người dùng (các loại trình duyệt), khi truy cập, sẽ tránh tạo phiên ID cho các trình duyệt thuộc loại được liệt kê trong spiders.txt
    - Robots.txt : Hạn chế phần nào đó trên trang web mà robot có thể truy cập, kiểm soát quyền truy cập mà robot có thể thực thi được. 
- Map execution paths through application.(Mục 7)
---
**2.Mục đích**
- Tìm ra các thông tin bị rò rỉ của các đường dẫn ứng dụng Web hoặc các đường dẫn thư mục (folder paths). 
- Tạo danh sách các thư mục mà nó tránh được Spiders, Robots, Crawlers. 
---
**3.How to test**
 ***hyperlink***
- Liên kết hàng nghìn tỷ trang và tệp với nhau. 
- VD : Computer Hope home page là một hyperlink. 
- Cho phép các trang Web kết nối với các trang Web khác, từ trang này có thể truy cập trang khác mà không cần phải nhập URL của trang web đó. 
- Được tạo bằng HTML. Hoặc XML với các liên kết XML. 
***Robots Exclusion Protocol***
- Tiêu chuẩn loại trừ Robots (robots.txt)
- Tiêu chuẩn được các Web áp dụng để giao tiếp với web crawler và các web robots khác. Chỉ ra, thông báo khu vực nào của Web không được xử lý hoặc không được quét. 
---
***a. robots.txt***
- Web Spiders, Robots, Crawlers : Truy xuất 1 trang Web, duyệt qua tất cả các Hyperlink một cách đệ quy để truy xuất thêm nội dung các trang Web khác. 
- Những hành động được chấp nhận của các Web đó, được chỉ định bởi Robots Exclusion Protocol(robots.txt) trong thư mục root. 
- Ví dụ : Tệp robots.txt từ Facebook 
![Robots.txt của Facebook](https://i.imgur.com/0Tr3m7s.jpg)
- Lệnh Disallow chỉ định những source nào bị cấm bởi các trình spiders, robots, crawler. 
- Trong đó, User-Agent đề cập đến các trình spiders, robots, crawler. 
- User-Agent:* sẽ áp dụng cho tất cả các trình spiders, robots, crawler được trích dẫn dưới nó : 
![](https://i.imgur.com/xsSEwUW.jpg)
- Trong giáo trình có đề cập, không nên coi robots.txt là một cơ chế thực thi các lệnh cấm, hạn chế về cách mà nội dung web được truy cập., lữu trữ hoặc tái bản bởi bên thứ 3.

***b. robots.txt in Webroot - with 'Wget' or 'Curl'***
- Truy xuất từ thư mục web root của web server. 
![](https://i.imgur.com/E5O99mW.jpg)
- Kết quả khi dùng Wget và Curl là như nhau, cùng trả về User-Agent : *

***c. robots.txt in Webroot - with 'Rockspider'***
- rockspider tự động tạo phạm vi ban đầu cho các trình spiders, robots, crawler và thư mục của 1 website nào đó. 
![](https://i.imgur.com/GrybNuE.jpg)
- Tự động tải xuống file robots.txt với đường dẫn như trên. 
- Lưu lại file "robots.txt" theo cú pháp ở bước 2. 
- Tự động tạo phạm vi ban đầu dựa trên Allow : các URIs từ trang google.com có địa chỉ loop back là 127.0.0.1 với port 8080  - yêu cầu phần mềm kết nối với server thông qua web browser. 

***d. Phân tích tệp robots.txt bằng Google Webmaster Tools***
- Đăng nhập. Link : https://www.google.com/webmasters/tools/robots-testing-tool
- Trên bảng điều khiển, viết URL của trang web sẽ được phân tích. 
- Chọn giữa các phương thức sẵn có và làm theo hướng dẫn. 
- Hướng dẫn tìm được : 
    - Mở công cụ Tester Tool.
    - Tìm tới robots.txt
    - Tìm các cảnh báo cú pháp và lỗi logic. 
    - Nhập URL của trang web vào hộp văn bản ở cuối trang. 
    - Chọn User-Agent mà mình muốn mô phỏng trong danh sách thả xuống. 
    - Press "TEST" -> Kiểm tra quyền truy cập. 
    - Kiểm tra xem TEST có hiện nội dung ACCEPTED hay BLOCKED không, URL đã nhập có bị hcặn bởi Google web crawlers không ? 
    - Chỉnh sửa file và kiểm tra nếu cần.
    - Sao chép thay đổi ban nãy vào robots.txt
    - Công cụ này chỉ kiểm tra bản nháp mà không ảnh hưởng đến file thực tế trên trang web mình test. 

---
***META TAG***
- Đặt trong phần Headertrong file HTML
- META Tags với Burp 
    - Dựa trên các Disallow được liệt kê trong robots.txt 
    - Tìm biểu thức cho <META NAME="ROBOTS"


- Ví dụ demo em chưa hiểu 
---
***TOOLS***
- Browser (View Source function)
- Curl với Wget trong Webroot.
- Rockspider trong Webroot. 
---
***HOW to Verify and Test Robots.txt file by Python***
a. Kiểm tra xem robots.txt có tồn tại không ?
```python 
import requests
def status_code(url):
    r = requests.get(url)
    return r.status_code

print(status_code('https://tên-miền/robots.txt'))
```
- Kết quả trả về status_code = 200 : Success. Có tồn tại robots.txt

b. Xem file robots.txt
```python
import re
from urllib.request import urlopen
robots = 'https://softhints.com/robots.txt'

sitemap_ls = []

with urlopen(robots) as stream:
    for line in urlopen(robots).read().decode("utf-8").split('\n'):
        if 'Sitemap'.lower() in line.lower():
            sitemap_url = re.findall(r' (https.*xml)', line)[0]
            sitemap_ls.append(sitemap_url)
```
- Trong trường hợp có bảo mật, sẽ trả về lỗi 403(Bị cấm)

c. Trích xuất sitemap 
from urllib.request import urlopen, urlparse
import re

test_url = "https://blog.softhints.com"

def get_robots(test_url):

    domain = urlparse(test_url).netloc
    scheme = urlparse(test_url).scheme
    robots =  f'{scheme}://{domain}/robots.txt'

    sitemap_url = ''
    
    sitemap_ls = []

    with urlopen(robots) as stream:
        for line in urlopen(robots).read().decode("utf-8").split('\n'):
            if 'Sitemap'.lower() in line.lower():
                sitemap_url = re.findall(r' (https.*xml)', line)[0]
                sitemap_ls.append(sitemap_url)
    return list(set(sitemap_ls))

get_robots(test_url)


